# Reddit Award PredictorPredict the likelihood that a post receives an award on the social media website Reddit. Focus primarily on making predictions early in a post's life-cycle. ## <ins> Technologies/Packages Used </ins>* Python	* Pandas	* Numpy	* Scikit-Learn	* XGBoost	* Spacy	* PRAW* AWS	* Lambda	* StepFunction	* CloudWatch	* S3	* RDS## <ins> Methods Used </ins>* ETL* Data Visualization* Hypothesis Testing* Machine Learning* Cross Validation* Hyperparameter Tuning## <ins> Motivation <ins>Internet fame is a fickle thing, influenced by current trends and requiring a whole lot of luck when it comes to being in the right place in the right time. While the chaotic human element may be above the ability to predict, there are enough mundane features that go in to a social media post that its chance of success can be estimated in advance.On Reddit users across the globe work together to curate content across a variety of smaller communities across the website, called "subreddits." Successful posts gain many "upvotes" from the userbase, causing the post to rise and gain more visibility, which in turn tends to lead to more upvotes and more rising. In addition to voting for, or commenting on, a post Reddit users may also choose to cash in some 'coins' and provide that post with an award. Users can gain these coins by having their previous content awarded or by paying real-world money for them. Among the most prestigious of these awards are "gold" and "platinum," and a post which has received one of these is then considered "gilded."Why might this be a useful thing to do? One particularly effective method of advertisement is the embedding of a product, or brand name, subtly inside of media - both social and otherwise. When a Reddit user comments about a particularly positive experience with a particular brand there is the very real possibility that they are not a happy consumer but instead a part of the marketing world trying to increase brand exposure to a large, and unwitting, audience. User testimonials are great advertisements after all, assuming that we believe the source to be "real."  In building this type of marketing campaign it is cost-effective to focus efforts on inserting the advertisement into online discussions which will garner large amounts of attention. That is, focusing on Reddit posts which are likely to gain a large number of upvotes and to rise to the top. Simply knowing which posts are doing well, however, is not enough - these posts can garner hundreds, if not thousands, of comments. Any new comment added after popularity is achieved will simply be lost in the mix. Rather, it is important to know *in advance* which posts are likely going to do well. Comments on young posts stand out more and have more opportunities to gather upvotes and thus rise to the top - which means more viewers. Of course the average Reddit user may also be interested in this tool as a simple opportunity to gain more upvotes and awards themselves - useless online currency that is nonetheless hoarded in a manner not unlike a Dragon jealously gathering mounds of treasure.In this case the act of gilding is considered as a proxy for post success. Interestingly whether or not a post is gilded is not strongly correlated to the final number of upvotes or comments it receives (Pearson coefficient r = 0.06 and 0.05 respectively) although these two features *are* correlated with one another (r = 0.6). Thus the presence of a gilding can be considered as a totally separate dimension of post success when compared to total number of upvotes.  The output predictions (probability of being gilded) of the final model does have a slightly higher correlation with total upvotes than does actual gilding (r = 0.1), indicating that it should be possible to train a regression algorithm on these same/similar features in order to predict success in this dimension a well.Given that only about 1% of all posts across the considered subreddits receive a gilding, the final problem can be considered as a heavily-imbalanced binary classification.As a last point, the heavy reliance on AWS serverless architecture was a particular emphasis so as to minimize costs and to ensure scalability.## <ins> The Data </ins>Broadly, the gathered data can be split in to one of two categories:1. Post Metadata2. User Engagement DataPost Metadata are those immutable parameters such as title, time the post was made, whether a post contained text, images, or video, what subreddit it is in, etc. In contrast User Engagement Data includes those things which are time-dependent, such as the number of upvotes and/or comments a post has. This latter type of data is captured only at a single instance in time, a snapshot of the post at a particular "age" (the time since posting). Whenever possible, this Engagement data is normalized by dividing by the post's age at the time of scraping so as to account for the potential imbalance between newer posts and older ones. The particular features collected for each post:#### Boolean:* *contest_mode* - whether the post is in contest mode wherein comment order is randomized and no upvotes are shown on comments* *edited* - whether the post was edited after publishing* *adult_content* - whether the post is intended for users 18+ in age* *oc* - whether the post is original content by the user (as opposed to re-posting others' content)* *reddit_media* - whether the post is reddit media (as opposed to outside media or text)* *selfpost* - whether the post is text-only (may still contain media links embedded within)* *video* - whether the post is a video link* *distinguished* - whether the post was made by a moderator and chosen to be distinguished#### Categorical:* *content_categories* - category of post, one of ['None,' 'photography,' 'drawing_and_painting,' 'writing,' 'diy_and_crafts']	* Over 91% of all posts fall in to "None" category* *subreddit* - which subreddit the post is made in	* The subreddits which are to be included in the data gathering can be modified in the file: *main/get_data/subreddits.txt** *how_sorted* - how Reddit was sorted when the post was scraped, one of "hot" or "new." This is not useful for predictions, but is gathered for data validation purposes.#### Numerical:* *upvotes* - the total number of upvotes at the time of scraping* *upvote_ratio* - the percentage of upvotes from all votes on the submission* *crossposts* - the number of other subreddits the same post has been submitted to* *comments* - the total number of comments the post has accumulated at the time of scraping* *post_age* - how many minutes ago was the post submitted, measured at the time of scraping* *upvote_rate* - *upvotes* divided by *post_age*	* Meant to account for the fact that older posts simply have more time to accumulate total upvotes* *comment_rate* - *comments* divided by *post_age*	* Meant to account for the fact that older posts simply have more time to accumulate total upvotes* *avg_up_rate* - the average rate at which the post's top comments are accumulating upvotes	* Top 15 comments, ordered by number of upvotes, included in analysis	* Comment rate computed for each top comment, then averaged* *std_up_rate* - the variance in the rate at which the post's top comments are accumulating upvotes	* Top 15 comments, ordered by number of upvotes, included in analysis	* Comment rate computed for each top comment, then standard deviation taken* *gild_rate* - the rate at which the post's top comments are themselves being gilded (given gold or platinum awards)* *distinguished_rate* - the rate at which the post's top comments are made by a moderator acting in official capacity* *op_comment_rate* - the rate at which the post's top comments are made by the original post author* *premium_auth_rate* - the rate at which the post's top comments are made by authors which have themselves been recently gilded or have a paid subscription to reddit which gives them many coins to spend* *initial_silver* - the number of silver awards the post has received at the time of scraping 	* A cheaper award than either Gold or Platinum and much, much more common* *created_utc* - the exact time the post was submitted, in UTC epoch time* *gold_awarded* - how many gold the post had received 24 hours after being initially scraped* *platinum_awarded* - how many platinum the post had received 24 hours after being initially scraped* *final_upvotes* - the number of upvotes the post had received 24 hours after being initially scraped	* due to a modification the source code, the first few thousand posts did not have this value collected and now show as 'True' within the table* *final_num_comments* - the number of comments the post had received 24 hours after being initially scraped	* due to a modification the source code, the first few thousand posts did not have this value collected and now show as 'True' within the table#### Other:* *id* - a unique string for identifying the post consisting of two parts:	* the first 6 characters are Reddit's unique string for each post	* the last 11 characters include how Reddit was sorted when the post was scraped as well as the date and time of scraping	* a given post has the chance of being scraped more than once, so a unique Reddit post may show up two or more times within the analysis, necessitating the creation of a unique string for each instance* *title* - a string, the post's title## <ins> Results Summary </ins>Ultimately, an XGBoost model was chosen not only for its accuracy but also for its ease of implementation in AWS Sagemaker (TODO). Five-fold cross validation on 80% of the data was used for hyperparameter tuning. The metric used for success was the area under the precision-recall curve (PR-AUC). Due to the stochastic nature of the model, and the fact that data is still accumulating on a daily basis, the final metrics of the model fluctuate. On a hold-out set consisting of 20% of the data, the best models may achieve* A PR-AUC of 0.2 with an ROC-AUC of 0.87* 60% precision with 15% recall (when choosing decision threshold to prioritize precision)* 30% precision with 35% recall (when choosing the decision threshold such that 1% of posts are predicted to be gilded)<p align="center">	<img src="https://github.com/DillonNMorse/Reddit_Award_Predictor/blob/main/images/separation_of_posts_by_predicted_probability.png?raw=true"/></p>* Note on results - these best models use a max_depth of 7 for the weak learners (the trees) in the XGBoost model, they also include the post's titles encoded as 300 numerical columns via the SPACY word2vec embedding.	* By "best model" I mean they perform best when considering both five-fold cross validation within the training set (for hyperparameter tuning) *and* final PR-AUC on the testing set.	* Despite the title data adding 300 new columns of data, the XGBoost algorithm continues to select six of the above features as being more important than the most-important title-encoding feature.	* When comparing the PR curves generated on the training set to the curves generated on the testing set it is clear that these models are highly over-fit.	* Regularization does not mitigate this over-fitting, despite manually adjusting the *lambda* and *alpha* values upwards (which reduce PR-AUC on both the testing-folds within cross-validation, and are thus not naturally selected by hyperparameter tuning algorithms, and reduce scores on the testing set) and setting other hyperparameters to more conservative values. Attempts at bringing the scores on training set downwards in order to be comparable to scores on the testing set *always* tend to drive scores on the testing set down as well.	* Models with more conservative max depths (depth=3) and which do not include the title data have typical PR-AUC's on the order of 0.12 on the testing set, with precision maxing out around 40%. Although they do tend to have slightly higher ROC-AUC's of around 0.90.## <ins> Project Structure <ins>### Data CollectionAn AWS Step Function is triggered by AWS CloudWatch every 155 minutes. This sets off a sequence of AWS Lambda functions with Python 3.8 installed:1. *Pull Posts* - Using the Python package PRAW to interact with the Reddit API, in batches of 100 pull the most recent 250 posts from across the desired subreddits as well as the top 250 "rising" posts from these subreddits.	* Reddit's "rising" algorithm selects for success in much the same way I do, which could be an issue with experimental design. However, if sorting only by "new" there is very little user engagement data on any posts. The choice was made to consider an equal number of both, hopefully giving a balance which is still useful for any users accessing this tool "in the wild."2. *Retrieve Comment Data* - Interfacing directly with Reddit's API, iterate through the above 500 posts and retrieve data related to their comments.	* As far as efficiency goes, this is the bottleneck. Since all comments for a single post are returned in a given query, it is not possible to batch these requests.3. *Pause for Gilds* - Do nothing for 24 hours.4. *Fetch Gilds* - Using PRAW and working in batches of 100, query the final number of gildings, upvotes, and comments these posts have received. Pickle and save file to AWS S3 staging bucket.	* While these values may continue to change, most posts have shelf-lives of less than one day. These features are near their *ultimate final values.![Get Data StepFunction in AWS](https://github.com/DillonNMorse/Reddit_Award_Predictor/blob/main/images/get_data_stepfunctions_graph.png?raw=true)![Move Data StepFunction in AWS](https://github.com/DillonNMorse/Reddit_Award_Predictor/blob/main/images/move_data_stepfunctions_graph.png?raw=truee)### Data Structuring and StorageAn AWS Step Function is triggered by AWS CloudWatch every morning. This sets off a sequence of AWS Lambda functions with Python 3.8 installed:1.  *Get all file names* - Query AWS S3 staging bucket to populate a list of all files set for processing2. *Add top file to database* - Read top file, transform data to match SQL table schema, append data to table3. *Was there an error?* - Check for exceptions in previous step4. *Move file to error folder* - Move pickled data file from staging directory to a separate "error" directory if an exception occurred5. *Move file to completed folder* - Move pickled data file from staging directory to a separate "completed" directory if an exception occurred6. *Is list empty?* - Check length of list of files that need to be processed7. *Done* - Terminate function if list is empty###  Data AccessThe AWS RDS Aurora Serverless PostgreSQL database has a limit return size of 1mb per query, which falls far short of the roughly 44mb (and growing) of useful features now collected. Thus the query must be subdivided in to smaller parts. 1. Query table to retrieve the set of all unique first-five-characters from the "id" column, these contain the unique tags associated with each data-scraping event.2. Group scraping tags such that the set of all posts associated with these tags contain less than 1mb of data	* This was a trial-and-error process, settled around 12 scraping events or roughly 3000 Reddit posts3. Iterate over all scraping tag groups, query only those posts which fall in to a particular group4. Concatenate all groups' data in to a single Pandas dataframe.### Data Exploration and Model Construction1. Check that data is equally distributed over time to avoid potential gaps in performance.2. Check all distributions, do p-tests on each feature to determine which ones have potential explanatory power when it comes to predicting the likelihood of gilding.	* Here the data is binned and a null-hypothesis is put forward: the fraction of posts in each bin which are gilded is the same across all bins. 	* That is, the prior probability of gilding (or the percentage of posts which have thus far been gilded) does *not* depend on the value that this particular feature takes on.3. Build train/test splits and construct pipeline which allows for experimentation during model building.	* Pipeline allows for automated setting of the following features using Scikit-Learn fit/transform/predict methods:		* Oversampling of minority class (via SMOTE)		* Undersampling of majority class (via SMOTE)		* Amount of smoothing of target-encoded categorical variables		* Newly constructed features to be included, scraped features to be dropped		* Encoding of the Reddit post title strings		* The amount of information to retain from these strings via PCA (if any)4. Backwards recursive feature selection, dropping the worst-performing feature (measured by PR-AUC) at each step.	* Greedy algorithm. Could improve performance drastically by simply checking feature importance at each step.5. Evaluation of model on a hold-out (testing) set, 20% (or around 35,000 instances) of the total data set.<p align="center">	<img src="https://github.com/DillonNMorse/Reddit_Award_Predictor/blob/main/images/model_metrics.png?raw=true"/></p>### DeploymentTODO - deploy on AWS SageMaker. Simplest approach is to train the model locally then push to SageMaker. A more sophisticated approach might set up another StepFunction which regularly queries the SQL table, builds and encodes all features, then writes to an S3 bucket where SageMaker can re-train on the data. Set up API to query the model with a Reddit post url. To finish, track incoming data, predictions made on that data, and 24-hour delayed post outcomes to continually monitor model performance as well as the input data distribution, checking for drift. 